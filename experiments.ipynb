{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amd\\anaconda3\\envs\\mktmp\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:65: UserWarning: Specified provider 'DnnlExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  {'input_ids': array([[   0,  100,  524,  182, 1372,    2]], dtype=int64), 'attention_mask': array([[1, 1, 1, 1, 1, 1]], dtype=int64)}\n",
      "1) joy 0.9179\n",
      "2) optimism 0.0499\n",
      "3) sadness 0.0225\n",
      "4) anger 0.0097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Importing Necessary modules\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer,RobertaTokenizer,TextClassificationPipeline\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from fastapi import FastAPI\n",
    "from scipy.special import softmax\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "import uvicorn\n",
    "import urllib\n",
    "import csv\n",
    "\n",
    "def preprocess(tweets):\n",
    "    ptweets = []\n",
    "    for tweet in tweets :\n",
    "        new_text = []\n",
    "        for t in tweet.split(\" \"):\n",
    "            t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "        new_text = \" \".join(new_text)\n",
    "        ptweets.append(new_text)\n",
    "    return ptweets\n",
    "\n",
    "model_path = 'EmoTwitter/app/twitter-models/base/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#The models are optimized for CPU inference, therefore no support for GPU execution is provided as of now\n",
    "ort_session = ort.InferenceSession(model_path+'/model.onnx',providers=['DnnlExecutionProvider'])\n",
    "\n",
    "text = [\"I am very happy\"]#,\"Very very disaponted\"]\n",
    "text = preprocess(text)\n",
    "\n",
    "inputs = tokenizer(text, add_special_tokens=True,return_tensors=\"np\")\n",
    "\n",
    "#convint = map()\n",
    "inputs = {k: v.astype(np.int64) for k,v in inputs.items()} # handling in windows data type size\n",
    "print('input : ',inputs)\n",
    "\n",
    "\n",
    "outputs_name = ort_session.get_outputs()[0].name\n",
    "outputs = ort_session.run(output_names=[outputs_name], input_feed=inputs)\n",
    "scores = outputs[0][0]\n",
    "\n",
    "labels = ['anger', 'joy', 'optimism', 'sadness']\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "scores = softmax(scores)\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  100,  524,  182, 1372,    2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amd\\anaconda3\\envs\\mktmp\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:65: UserWarning: Specified provider 'DnnlExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1.7885597 ,  2.7592015 , -0.15227139, -0.95142317]],\n",
      "      dtype=float32)]\n",
      "1) joy 0.9179\n",
      "2) optimism 0.0499\n",
      "3) sadness 0.0225\n",
      "4) anger 0.0097\n",
      "[array([[-0.5423151, -1.2919993, -1.5097198,  3.461708 ]], dtype=float32)]\n",
      "1) sadness 0.9673\n",
      "2) anger 0.0176\n",
      "3) joy 0.0083\n",
      "4) optimism 0.0067\n"
     ]
    }
   ],
   "source": [
    "text = [\"I am very happy\",\"Very very disaponted\"]\n",
    "text = preprocess(text)\n",
    "labels = ['anger', 'joy', 'optimism', 'sadness']\n",
    "\n",
    "\n",
    "model_path = 'EmoTwitter/app/twitter-models/base/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#The models are optimized for CPU inference, therefore no support for GPU execution is provided as of now\n",
    "ort_session = ort.InferenceSession(model_path+'/model.onnx',providers=['DnnlExecutionProvider'])\n",
    "\n",
    "for t in text :\n",
    "    inputs = tokenizer(t, add_special_tokens=True,return_tensors=\"np\")\n",
    "    inputs = {k: [vi.astype(np.int64) for vi in v] for k,v in inputs.items()} # handling in windows data type size\n",
    "    outputs_name = ort_session.get_outputs()[0].name\n",
    "    outputs = ort_session.run(output_names=[outputs_name], input_feed=inputs)\n",
    "    print(outputs)\n",
    "\n",
    "    scores = outputs[0][0]\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    scores = softmax(scores)\n",
    "    opack = {}\n",
    "    preds = {}\n",
    "    opack['label'] = (labels[0],scores[0])\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = labels[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        opack['logits'][l] = np.round(float(s), 4)\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "    print(\"opakc \",opack)\n",
    "    \n",
    "#convint = map()\n",
    "#print(inputs)\n",
    "#inputs = {k: [vi.astype(np.int64) for vi in v] for k,v in inputs.items()} # handling in windows data type size\n",
    "#print('input : ',inputs)\n",
    "\n",
    "#outputs_name = ort_session.get_outputs()[0].name\n",
    "#outputs = ort_session.run(output_names=[outputs_name], input_feed={'input_ids': [[   0,  100,  524,  182, 1372,    2]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]})\n",
    "#outputs = ort_session.run(output_names=[outputs_name], input_feed=inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mktmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
